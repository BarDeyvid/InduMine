"""
This type stub file was generated by pyright.
"""

import sentencepiece as spm
from pathlib import Path
from typing import List

class Tokenizer:
    def encode(self, sentence: str) -> List[str]:
        ...
    
    def decode(self, tokens: List[str]) -> str:
        ...
    


class SentencePieceTokenizer(Tokenizer):
    def __init__(self, model_file: Path) -> None:
        ...
    
    def lazy_processor(self) -> spm.SentencePieceProcessor:
        ...
    
    def encode(self, sentence: str) -> List[str]:
        ...
    
    def decode(self, tokens: List[str]) -> str:
        """
        # Returns not decoded byte-fallback tokens, quite detrimental to Asian languages translations
        detokenized = "".join(tokens)
        return detokenized.replace("â–", " ")
        """
        ...
    


class BPETokenizer(Tokenizer):
    def __init__(self, model_file: Path, from_code: str, to_code: str) -> None:
        ...
    
    def lazy_load(self): # -> None:
        ...
    
    def encode(self, sentence: str) -> List[str]:
        ...
    
    def decode(self, tokens: List[str]) -> str:
        ...
    


